# ニューラルネットワーク
正解ラベルはone-hotエンコーディングに変換する
2 = [0, 0, 1, 0, ...]

ポラニーのパラドックス

### シグモイド関数
![alt](https://latex.codecogs.com/gif.latex?\sigma&space;(x)&space;=&space;\frac{1}{1&plus;e^{-x}})
シグモイドは、実数の値を0~1の範囲に写像する。
0付近では勾配が急で、非常に小さい値と大きい値の両側で曲線は平坦になる。

```
def sigmoid_double(x):
  return 1.0 / (1.0 + np.exp(-x))

def sigmoid(z):
  return np.vectorize(sigmoid_double(z))
```

### ロジスティック回帰
https://qiita.com/yshi12/items/3dbd336bd9ff7a426ce9
線形回帰と似ているが、目的変数が2値のときに利用する。

### 線形回帰
https://qiita.com/ynakayama/items/5732f0631c860d4b5d8b



## 基礎
y = σ(Wx + b)


## 順伝播型ネットワーク
アフィン線形変換
https://qiita.com/koshian2/items/c133e2e10c261b8646bf

## 損失関数
予測がどれだけ目的を達成できなかったかを定量化。
目的関数とも呼ぶ。

![alt](https://latex.codecogs.com/gif.latex?\Sigma&space;Loss(W,&space;b;&space;Xi,&space;\hat{yi}))

訓練の目標は、パラメータを適合させるための良い戦略を見つけることによって、損失を最小化すること。

### 平均二乗誤差
![alt](https://latex.codecogs.com/gif.latex?MSE(y,&space;\hat{y})&space;=&space;\frac{1}{2}\sum_{i&space;=&space;1}^{k}(y_{i}&space;-&space;\hat{y}_{i})^{2})

ラベル = ^y = ^y1, ^y2 ....
予測   = y = y1, y2 ....

## 損失関数の極小値を求める
損失を最小化するためには、微分を計算して０にする必要がある。
微分が０になる点でのパラメータの組を解と呼ぶ。
関数の微分を計算し、特定の点で評価することを勾配の計算と呼ぶ。

### 極小値を見つけるための勾配降下法
確率的勾配降下法(SGD)


### ネットワークへの勾配の逆伝播





## まとめ
* シーケンシャルネットワーク
層を直線状に積み重ねることで構築された単純な人口ニューラルネットワーク。
画像認識など、様々な機械学習の問題にニューラルネットワークを適用できる。

* 順伝播型ネットワーク
活性化関数を備えた全結合層からなるシーケンシャルネットワーク。

* 損失関数
予測の質を評価し、平均二乗誤差は実際に使用される一般的な損失。
損失関数を使用すると、モデルの精度を定量的に評価できる。

* 勾配降下法
関数を最小化するためのアルゴリズム。
勾配降下は、関数の最も急な勾配をたどることで行う。
機械学習では勾配降下法を使用して、損失が最小になるモデルの重みを見つける。

* 確率的勾配降下法
勾配降下アルゴリズムを変形したもの。
確率勾配降下法では、ミニバッチごとに勾配を計算して、ネットワークのパラメータを更新する。

大規模な訓練セットでは、確率勾配降下は通常、通常の勾配降下法よりはるかに高速。

* 遂次ニューラルネットワーク
逆伝播アルゴリズムを使用して勾配を効率的に計算できる。
逆伝播とミニバッチの組み合わせにより、巨大なデータセットでの訓練が実用的な速さで行える。


